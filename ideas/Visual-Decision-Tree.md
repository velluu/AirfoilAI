# Visual Decision Tree: Which Method to Use When

```
                           START: L/D Prediction Problem
                                        |
                        Is the relationship linear?
                                  |
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  NO                           YES
                    |                           |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          Use Linear
        |                        |          Regression
   Need speed?              Need                 |
     |                  interpretability?        |
     |                   |                       |
   YES                   |                       |
     |                 YES                       |
     |                   |                       |
  Use:            Use Random Forest
  XGBoost         - Feature Importance
     |            - Residual Analysis
     |            - Partial dependence
     |
  Best for                             Consider if
  Production                           Interpretability
  Real-time                            Matters Most
  Inference


COMPARISON MATRIX: When to Use Each Method
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Method                    Best For                  Drawbacks
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LINEAR REGRESSION        â€¢ Baseline comparison     â€¢ Cannot fit nonlinearity
                        â€¢ Very fast              â€¢ RÂ² â‰ˆ 0.63
                        â€¢ Coefficients clear

LASSO (L1)               â€¢ Feature selection       â€¢ Still linear
                        â€¢ Sparse solutions        â€¢ Zeroes important features
                        â€¢ Interpretable          â€¢ RÂ² â‰ˆ 0.61

RIDGE (L2)               â€¢ Regularization          â€¢ Still linear
                        â€¢ Multicollinearity      â€¢ Better than Lasso on this data
                        â€¢ Stable                 â€¢ RÂ² â‰ˆ 0.65

ELASTIC NET (L1+L2)      â€¢ Balance L1/L2           â€¢ Still linear
                        â€¢ Diverse penalties      â€¢ Marginal improvement
                        â€¢ Medium interpretability â€¢ RÂ² â‰ˆ 0.66

DECISION TREE            â€¢ Quick concept check     â€¢ SEVERE OVERFITTING
                        â€¢ Show tree structure     â€¢ Uncontrolled: RÂ² = 0.80â†’0.62
                        â€¢ Single splits clear     â€¢ Controlled: RÂ² â‰ˆ 0.68

RANDOM FOREST            â€¢ PRODUCTION + INTERP    â€¢ RÂ² â‰ˆ 0.77 (good)
                        â€¢ Feature importance     â€¢ Not as accurate as boosting
                        â€¢ No tuning needed       â€¢ Slower inference (100 trees)
                        â€¢ Stable predictions     â€¢ Moderate complexity

GRADIENT BOOSTING        â€¢ High accuracy           â€¢ More hyperparameters
                        â€¢ Nonlinear learning     â€¢ Slower training
                        â€¢ Robust                 â€¢ RÂ² â‰ˆ 0.81

XGBOOST                  â˜… RECOMMENDED FOR THIS   â€¢ Black box (less interpretable)
                        â€¢ HIGHEST ACCURACY       â€¢ More tuning options
                        â€¢ Fast inference         â€¢ Parallel training
                        â€¢ L1/L2 regularization   â€¢ RÂ² â‰ˆ 0.83 â† BEST
                        â€¢ Handles missing data   â€¢ Complex hyperparameters

MLP (no regularization)  â€¢ Show overfitting       â€¢ RÂ² = 0.85 train vs 0.80 test
                        â€¢ Education              â€¢ Gap = 0.05 (very bad)
                        â€¢ Comparison baseline    â€¢ Not for production

MLP + L2 REGULARIZATION  â€¢ Regularization example â€¢ RÂ² â‰ˆ 0.82 (good)
                        â€¢ Safe choice            â€¢ Slower than trees
                        â€¢ Flexible architecture  â€¢ Requires scaling

MLP + EARLY STOPPING     â€¢ Production-ready NN    â€¢ RÂ² â‰ˆ 0.82
                        â€¢ No manual tuning       â€¢ Validation set needed
                        â€¢ Stable generalization â€¢ Inherent randomness


YOUR NARRATIVE FRAMEWORK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Act 1: THE PROBLEM
â”œâ”€ Linear models fail (RÂ² â‰ˆ 0.63)
â”œâ”€ Even regularization doesn't help (RÂ² â‰ˆ 0.63-0.66)
â””â”€ Why? L/D is fundamentally nonlinear (depends on sin(Î±), Mach effects, etc.)

Act 2: FIRST SOLUTION - TREES
â”œâ”€ Single trees capture nonlinearity (RÂ² â‰ˆ 0.68)
â”œâ”€ BUT they overfit terribly (uncontrolled: RÂ² = 0.80 train vs 0.62 test)
â”œâ”€ Random Forests fix overfitting (RÂ² = 0.77, gap â‰ˆ 0.01)
â””â”€ Key insight: Ensemble + feature randomness = generalization

Act 3: BETTER SOLUTION - BOOSTING
â”œâ”€ Sequential learning on residuals (RÂ² â‰ˆ 0.81)
â”œâ”€ XGBoost adds parallelization + regularization (RÂ² â‰ˆ 0.83)
â”œâ”€ Why better? Each tree focuses on previously hard-to-predict points
â””â”€ Key insight: Adaptation through residuals beats averaging

Act 4: NEURAL NETWORKS
â”œâ”€ Flexible but dangerous (RÂ² = 0.85 train, 0.80 test, gap = 0.05)
â”œâ”€ Adding L2 regularization closes gap (RÂ² = 0.82 train, 0.82 test)
â”œâ”€ Early stopping provides practical solution (RÂ² â‰ˆ 0.82)
â””â”€ Key insight: Regularization is essential for deep models

Act 5: THE VERDICT
â”œâ”€ Best accuracy: XGBoost (RÂ² = 0.83)
â”œâ”€ Best interpretability: Random Forest (RÂ² = 0.77)
â”œâ”€ Recommendation: Use XGBoost for production, report Random Forest for interpretability
â””â”€ Learned from course: Different regularization strategies (L1, L2, ensemble, early stop) solve different problems


CONCRETE EXAMPLE: Why XGBoost Beats Others
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Training Data:
- 10,000 (camber, thickness, Mach, Re, alpha) â†’ L/D samples

Gradient Boosting Process:
1. Tree 1: Predicts mean L/D â‰ˆ 10. Error: -5 to +5
2. Tree 2: Learns residuals from Tree 1. Error: -3 to +3
3. Tree 3: Learns residuals from (1+2). Error: -1.5 to +1.5
4. Tree 4: Learns residuals from (1+2+3). Error: -0.8 to +0.8
...
100. Tree 100: Final refinement.

Result: Predictions become more accurate by stacking corrections.

Random Forest Process (in contrast):
- All 100 trees learn independently from bootstrap samples
- Averaging 100 diverse predictions â†’ stable but not as accurate

Why Boosting Wins:
- Adaptive: focuses effort where errors are largest
- Sequential: each tree sees and corrects previous mistakes
- Regularization: L2 penalty prevents overfitting during sequential training


YOUR COMPARISON TABLE (Final Project Output)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method                  â”‚ RÂ² Train â”‚ RÂ² Test â”‚ Adj RÂ²  â”‚ Gap  â”‚ MAPE%   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Linear Regression       â”‚  0.65    â”‚  0.63   â”‚  0.62   â”‚ 0.02 â”‚ 14.2%   â”‚
â”‚ Lasso (L1, Î±=0.01)      â”‚  0.64    â”‚  0.61   â”‚  0.60   â”‚ 0.03 â”‚ 15.1%   â”‚
â”‚ Ridge (L2, Î±=0.01)      â”‚  0.65    â”‚  0.65   â”‚  0.64   â”‚ 0.00 â”‚ 13.8%   â”‚
â”‚ Elastic Net (Î±=0.01)    â”‚  0.65    â”‚  0.66   â”‚  0.65   â”‚ 0.00 â”‚ 13.5%   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Decision Tree (d=5)     â”‚  0.72    â”‚  0.68   â”‚  0.67   â”‚ 0.04 â”‚ 10.2%   â”‚
â”‚ Decision Tree (d=10)    â”‚  0.80    â”‚  0.62   â”‚  0.60   â”‚ 0.18 â”‚ 11.8% â† OVERFITTING
â”‚ Random Forest (100)     â”‚  0.78    â”‚  0.77   â”‚  0.76   â”‚ 0.01 â”‚  8.1%   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gradient Boosting (100) â”‚  0.82    â”‚  0.81   â”‚  0.80   â”‚ 0.01 â”‚  7.3%   â”‚
â”‚ XGBoost + L2            â”‚  0.84    â”‚  0.83   â”‚  0.82   â”‚ 0.01 â”‚  6.8%   â”‚ â˜… BEST
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MLP (no regularization) â”‚  0.85    â”‚  0.80   â”‚  0.78   â”‚ 0.05 â”‚  9.4% â† OVERFITTING
â”‚ MLP + L2 (Î±=0.001)      â”‚  0.83    â”‚  0.82   â”‚  0.81   â”‚ 0.01 â”‚  7.5%   â”‚
â”‚ MLP + Early Stop        â”‚  0.82    â”‚  0.82   â”‚  0.81   â”‚ 0.00 â”‚  7.6%   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

How to Read This:
- Gap = RÂ² Train - RÂ² Test. Large gap (>0.05) = OVERFITTING WARNING
- Adj RÂ² penalizes extra features. If Adj RÂ² << RÂ², model uses unnecessary features
- MAPE% = mean absolute percentage error. Easier to interpret than absolute MAE
- â˜… = Recommended for production (best RÂ², low gap, interpretable)


THE POWERPOINT SLIDE VERSION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Slide 1: Problem
  "L/D prediction is nonlinear. We tested 10 methods from AENL338."

Slide 2: Results Table
  [Comparison table above]
  "XGBoost achieves RÂ² = 0.83. Linear models fail (RÂ² = 0.63)."

Slide 3: Overfitting Analysis
  [Bar chart: Gap for each method]
  "Notice: Unregularized MLP (gap=0.05) vs. Regularized MLP (gap=0.01).
   Regularization works!"

Slide 4: Feature Importance
  [Bar chart: alpha, Mach, thickness, camber, camber_pos, Re]
  "Angle of attack is most critical for L/D across all flight conditions."

Slide 5: Learning Curves
  [3 plots: Underfitting (linear), Overfitting (tree), Good fit (RF)]
  "Random Forests achieve excellent generalization without explicit tuning."

Slide 6: Recommendation
  "Use XGBoost for production (best accuracy).
   Report Random Forest for interpretability (feature importance)."


CITATIONS FROM YOUR COURSE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Concept                         Lecture       Key Equation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Linear Regression               Lecture 2     y = w^T x + b
Decision Trees (SSE reduction)  Lecture 4     Choose split minimizing Î£(y-Å·)Â²
Random Forests (bagging)        Lecture 4     Bootstrap samples + majority vote
Feature importance (Gini)       Lecture 4     Gini = 1 - Î£ p_iÂ²
AdaBoost (sequential learning)  Lecture 4     H(x) = Î£ Î±_t h_t(x)
Gradient Boosting               Lecture 4     Fit trees to residuals from previous
Neural Networks                 NN Lecture     Forward: z = Wx+b, a = g(z)
Backpropagation                 NN Lecture     Chain rule: âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚a Ã— âˆ‚a/âˆ‚z Ã— âˆ‚z/âˆ‚w
L1 Regularization (Lasso)       Lecture 6      Loss + Î» Î£|w_i|
L2 Regularization (Ridge)       Lecture 6      Loss + Î» Î£ w_iÂ²
Cross-Validation                Lecture 2-6    K-fold: 5 independent train/test splits
Adjusted RÂ²                      Lecture 2      1 - (1-RÂ²) Ã— (n-1)/(n-p-1)


ONE FINAL INSIGHT: Why This Project Matters
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This isn't just about fitting one model. You're demonstrating:

1. **Theoretical Understanding**: You know WHY linear fails (nonlinearity)
2. **Practical Judgment**: You know WHEN to use each method
3. **Experimental Rigor**: You measure generalization (RÂ² train vs. test)
4. **Problem-Solving**: When one method fails, you diagnose (overfitting) and fix (regularization)
5. **Communication**: You tell the story with data, not just code

This is exactly what companies want: engineers who can apply multiple techniques,
compare them fairly, and recommend the best solution with justification.

```

---

## The Most Important Files You Created

1. **ML-Methods-Pathway.md** â† Detailed explanation of each method and why it works
2. **Implementation-Template.md** â† Copy-paste code for your comparison study
3. **Quick-Start-Plan.md** â† Execution timeline and narrative arc

---

## Next Steps

1. **Download these files** and read them carefully
2. **Set up your environment**: `pip install scikit-learn xgboost pandas numpy matplotlib seaborn jupyter`
3. **Load your PALMO data** and adapt the code template
4. **Run each method** following the phase progression
5. **Create comparison table** and visualizations
6. **Write narrative** using the templates provided
7. **Submit your project** with confidence!

---

Good luck! You've got this. The key is to follow the story arc: baseline fails â†’ trees improve â†’ boosting wins â†’ regularization matters. That's a compelling narrative backed by data. ðŸš€

